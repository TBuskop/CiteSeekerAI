{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB1Q2hDOeBMBbj_riNqglXYh2p5tBi8QWw\"\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "LIT_FILEPATH = \"merged_texts/paper_2_intro\"\n",
    "SYSTEM_PROMPT = \"system_prompt.txt\"\n",
    "\n",
    "# client = genai.Client(api_key=API_KEY)\n",
    "# response = client.models.generate_content(\n",
    "#     model=\"gemini-2.0-flash\", contents=\"Explain how AI works\"\n",
    "# )\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"Read content from a text file with error handling.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def start_chat_with_context(file_paths, initial_prompt=\"How many papers are in the supplied context?\", system_prompt=None):\n",
    "    \"\"\"Start a chat session with multiple text files as context and a system prompt.\n",
    "\n",
    "    Args:\n",
    "        file_paths: A list of file paths to use as context.\n",
    "        initial_prompt: The first user message/query about the text.\n",
    "        system_prompt: Instructions to set the model's behavior. If None, uses a default.\n",
    "\n",
    "    Returns:\n",
    "        The chat session object for continued conversation, or None on error.\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"Error: No file paths provided.\")\n",
    "        return None\n",
    "\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"You are a helpful assistant that analyzes the provided text.\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(MODEL)\n",
    "        chat = model.start_chat(history=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [system_prompt]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"model\",\n",
    "                \"parts\": [\"I will follow your instructions.\"]\n",
    "            }\n",
    "        ])\n",
    "\n",
    "        # Add context from each file as a separate message\n",
    "        for file_path in file_paths:\n",
    "            text_content = read_text_file(file_path)\n",
    "            if text_content:\n",
    "                chat.send_message(f\"Context from {file_path}:\\n{text_content}\")\n",
    "            else:\n",
    "                return None #Exit if file reading failed.\n",
    "\n",
    "        # Send the initial prompt after all context blocks\n",
    "        response = chat.send_message(initial_prompt)\n",
    "        print(f\"Model response:\\n{response.text}\")\n",
    "        return chat\n",
    "    except Exception as e:\n",
    "        print(f\"Error during chat initialization: {e}\")\n",
    "        return None\n",
    "\n",
    "def continue_chat(chat, user_message):\n",
    "    \"\"\"Continue the conversation with the model, with improved error handling.\"\"\"\n",
    "    if chat is None:\n",
    "        print(\"Error: Chat session is not initialized.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = chat.send_message(user_message)\n",
    "        print(f\"Model response:\\n{response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during chat continuation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['merged_texts/paper_2_intro\\\\paper_2_intro_merged_1.txt',\n",
       " 'merged_texts/paper_2_intro\\\\paper_2_intro_merged_2.txt',\n",
       " 'merged_texts/paper_2_intro\\\\paper_2_intro_merged_3.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "There are 9 papers referenced in the provided context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get filepaths\n",
    "file_list = [os.path.join(LIT_FILEPATH, file) for file in os.listdir(LIT_FILEPATH)]\n",
    "\n",
    "\n",
    "# file_list = [\"file1.txt\", \"file2.txt\", \"file3.txt\"] #replace with your file paths.\n",
    "chat_session = start_chat_with_context(file_list, system_prompt=SYSTEM_PROMPT)\n",
    "\n",
    "# if chat_session:\n",
    "#     continue_chat(chat_session, \"Can you provide a comparison between the documents?\")\n",
    "#     continue_chat(chat_session, \"What are the common themes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "The supplied context contains excerpts from 7 papers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if chat_session:\n",
    "    continue_chat(chat_session, \"How many papers are in the supplied context?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "The supplied context contains excerpts from the following papers:\n",
      "\n",
      "1. Alizadeh et al. - 2024 - Scenario analysis of local storylines to represent uncertainty in complex human-water systems.pdf\n",
      "2. Baulenas et al. - 2023 - Assembling the climate story use of storyline app.pdf\n",
      "3. Buskop et al. - 2024 - Amplifying exploration of regional climate risks .pdf\n",
      "4. Challinor et al. - 2018 - Transmission of climate risks across sectors and b.pdf\n",
      "5. Dottori et al. - 2018 - Increased human and economic losses from river flo.pdf\n",
      "6. Goulart et al. - 2024 - Compound flood impacts from Hurricane Sandy on New.pdf\n",
      "7. Haasnoot et al. - 2013 - Dynamic adaptive policy pathways A method for cra.pdf\n",
      "8. Leimbach et al. - 2023 - Structural change scenarios within the SSP framewo.pdf\n",
      "9. Lempert - 2013 - Scenarios that illuminate vulnerabilities and robust responses.pdf\n",
      "10. van der Wiel et al. - 2024 - KNMI'23 Climate Scenarios for the Netherlands Sto.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if chat_session:\n",
    "    continue_chat(chat_session, \"List the papers thate are in the supplied context?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academic_lit_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
